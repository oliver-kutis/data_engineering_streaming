{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa8e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set jvm options\n",
    "# os.environ[\"SPARK_SUBMIT_OPTS\"] = \"-Xmx4g -XX:+UnlockDiagnosticVMOptions -XX:+LogVMOutput -XX:LogFile=/tmp/jvm.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604fd3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/26 16:40:52 WARN Utils: Your hostname, mackutis.local resolves to a loopback address: 127.0.0.1; using 172.20.10.12 instead (on interface en0)\n",
      "25/01/26 16:40:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.4\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 17.0.14\n",
      "Branch HEAD\n",
      "Compiled by user yangjie01 on 2024-12-17T04:51:46Z\n",
      "Revision a6f220d951742f4074b37772485ee0ec7a774e7d\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b65ece-78ee-45c7-ac39-0ec9275a2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"--name\" \"PySparkShell\" \"pyspark-shell\"\n"
     ]
    }
   ],
   "source": [
    "!echo $PYSPARK_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9334e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/26 16:41:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Add the package dependency to run kafka\n",
    "spark_version = \"3.5.1\"\n",
    "scala_version = \"2.12\"\n",
    "# kafka_version = \n",
    "\n",
    "# packages = [\n",
    "#     f\"com.datastax.spark:spark-cassandra-connector_{scala_version}:{spark_version}\",\n",
    "#     f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}\"\n",
    "#     f\"org.apache.spark:spark-avro_{scala_version}:{spark_version}\",\n",
    "# ]\n",
    "# packages_str = \",\".join(packages)\n",
    "\n",
    "# https://stackoverflow.com/questions/58723314/pyspark-failed-to-find-data-source-kafka/58723724#58723724\n",
    "# https://stackoverflow.com/questions/31841509/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-p\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--conf spark.default.parallelism=2 --packages {packages_str} pyspark-shell\"\n",
    "# print(os.environ[\"PYSPARK_SUBMIT_ARGS\"])\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    # .master(\"http://172.19.0.3:7078\")\n",
    "    # .master(\"spark://localhost:7077\")\n",
    "    .appName(\"test_kafka_read_stream\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost:9042\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2603163f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m kafka \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mspark\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://broker:29092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage_view_events\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.4/libexec/python/pyspark/sql/streaming/readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.4/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.4/libexec/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "kafka = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    # .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"http://broker:29092\")\n",
    "    .option(\"subscribe\", \"page_view_events\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f844bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import (\n",
    "    window, col, date_format, unix_timestamp, from_json, approx_count_distinct\n",
    ")\n",
    "# from pyspark.sql import SchemaConverters\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('ts', LongType(), True),\n",
    "    StructField('sessionId', IntegerType(), True),\n",
    "    StructField('page', StringType(), True),\n",
    "    StructField('auth', StringType(), True),\n",
    "    StructField('method', StringType(), True),\n",
    "    StructField('status', IntegerType(), True),\n",
    "    StructField('level', StringType(), True),\n",
    "    StructField('itemInSession', IntegerType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('zip', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('userAgent', StringType(), True),\n",
    "    StructField('lon', DoubleType(), True),\n",
    "    StructField('lat', DoubleType(), True),\n",
    "    StructField('userId', IntegerType(), True),\n",
    "    StructField('lastName', StringType(), True),\n",
    "    StructField('firstName', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('registration', LongType(), True),\n",
    "    StructField('artist', StringType(), True),\n",
    "    StructField('song', StringType(), True),\n",
    "    StructField('duration', DoubleType(), True),\n",
    "])\n",
    "\n",
    "# jsonFormatSchema = open(\"../eventsim/schemas/page_view_events.avsc\", \"r\").read()\n",
    "# jsonFormatSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08425f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter = 0\n",
    "max_batches = 5\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    global batch_counter\n",
    "    batch_counter += 1\n",
    "    print(f\"Processing batch {batch_counter}\")\n",
    "\n",
    "    (\n",
    "        df\n",
    "        .selectExpr(\"CAST(value as STRING)\")\n",
    "        .select(from_json(\"value\", schema=schema).alias('value'))\n",
    "        .select(\"value.*\")\n",
    "    )\n",
    "    # # Collect and print the original DataFrame\n",
    "    # original_data = df.collect()\n",
    "    # print(\"Original DataFrame:\")\n",
    "    # for row in original_data:\n",
    "    #     print(row)\n",
    "\n",
    "    # # Collect and print the DataFrame after casting value to STRING\n",
    "    # df_str = df.selectExpr(\"CAST(value as STRING)\")\n",
    "    # string_data = df_str.collect()\n",
    "    # print(\"DataFrame after casting value to STRING:\")\n",
    "    # string_data = string_data.show(truncate=False)\n",
    "    # # for row in string_data:\n",
    "    # #     print(row)\n",
    "\n",
    "    # # Collect and print the DataFrame after parsing JSON\n",
    "    # df_json = df_str.select(from_json(col(\"value\"), schema).alias('value'))\n",
    "    # json_data = df_json.collect()\n",
    "    # print(\"DataFrame after parsing JSON:\")\n",
    "    # for row in json_data:\n",
    "    #     print(row)\n",
    "\n",
    "    if batch_counter >= max_batches:\n",
    "        query.stop()\n",
    "\n",
    "df_pageviews = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"page_view_events\") \\\n",
    "    .load()\n",
    "\n",
    "df_pageviews_str = df_pageviews.selectExpr(\"CAST(value as STRING) as value_string\", \"offset\")\n",
    "\n",
    "output = (\n",
    "    df_pageviews_str\n",
    "    # df_pageviews\n",
    "    # .select(\"value\")\n",
    "    .select(from_json(\"value_string\", schema=schema).alias('value_json'), \"offset\")\n",
    "    .select(\"value_json.*\", \"offset\")\n",
    "    .selectExpr(\"timestamp_millis(ts) as ts_timestamp\", \"*\")\n",
    "    .selectExpr(\n",
    "        \"to_date(ts_timestamp) as date\", \n",
    "        \"month(ts_timestamp) as month\",\n",
    "        \"year(ts_timestamp) as year\",\n",
    "        \"hour(ts_timestamp) as hour\",\n",
    "        \"minute(ts_timestamp) as minute\",\n",
    "        \"second(ts_timestamp) as second\", \n",
    "        \"*\")\n",
    "    # .select(\"ts_timestamp\", \"offset\")\n",
    ")\n",
    "\n",
    "output = (\n",
    "    output\n",
    "    .withWatermark(\"ts_timestamp\", \"60 seconds\")\n",
    "    .groupBy(\n",
    "        window(output.ts_timestamp, \"60 seconds\", \"10 seconds\"),\n",
    "        # \"minute\"\n",
    "    )\n",
    "    .count()\n",
    "    # .agg(\n",
    "    #     approx_count_distinct(\"sessionId\").alias(\"sessions\"),\n",
    "    # )\n",
    "    .withColumn(\"window_start\", date_format(col(\"window.start\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"unix_timestamp\", unix_timestamp(col(\"window_start\")))\n",
    "    .selectExpr(\"window_start as timestamp_minute\", \"unix_timestamp\", \"count\")\n",
    "    # .selectExpr(\"window\", \"window_start as timestamp_minute\", \"unix_timestamp\", \"count\")\n",
    "    # .sort(col(\"window\"), ascending=True)\n",
    "    # .count()\n",
    "    # .selectExpr(\"COUNT(distinct sessionId) as session_count\", \"minute\")\n",
    "    # .select(count_distinct(\"sessionId\").alias(\"session_count\"), \"minute\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    output\n",
    "    # df_pageviews\n",
    "    .writeStream\n",
    "    # .foreachBatch(process_batch)\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"console\")\n",
    "    # .format(\"org.apache.spark.sql.cassandra\")\n",
    "    # .options(table=\"metrics\", keyspace=\"spark_streaming\")\n",
    "    # .option(\"checkPointLocation\", \"/tmp/checkpoint/\")\n",
    "    # .options(table=\"page_views_test\", keyspace=\"spark_streaming\")\n",
    "    # .option(\"truncate\", \"false\")\n",
    "    # .option(\"numRows\", \"2\")\n",
    "    # .trigger(processingTime=\"1 minute\")\n",
    "    .trigger(processingTime=\"60 seconds\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66b8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
